{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Project : Enhancing Nucleus Segmentation and 3D Reconstruction Using Super-Resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "### Rita Sulaiman ‚Äì Student ID: 2210765051\n",
    "### Zeynep Yƒ±ldƒ±z ‚Äì Student ID: 2210765033\n",
    "### Zharasbek Bimagambetov ‚Äì Student ID: 2210356185\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load & Preprocess the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Image Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directory scaffold created under data\n",
      "‚úÖ Found 665 complete data tuples.\n",
      "üìä Split sizes ‚Äî Train: 531, Val: 67, Test: 67\n",
      "‚úÖ train saved: 531 samples\n",
      "‚úÖ val saved: 67 samples\n",
      "‚úÖ test saved: 67 samples\n",
      "train counts:  {'images': 531, 'masks': 531, 'distance_maps': 531, 'label_masks': 531, 'vague_masks': 531}\n",
      "val counts:  {'images': 67, 'masks': 67, 'distance_maps': 67, 'label_masks': 67, 'vague_masks': 67}\n",
      "test counts:  {'images': 67, 'masks': 67, 'distance_maps': 67, 'label_masks': 67, 'vague_masks': 67}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "# ================================\n",
    "# CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "RAW_ROOT = Path(r\"C:\\Users\\rita\\.cache\\kagglehub\\datasets\\ipateam\\nuinsseg\\versions\\5\")\n",
    "OUT_ROOT = Path(\"data\")\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "TARGET_SIZE = (256, 256)\n",
    "test_frac = 0.10\n",
    "val_frac = 0.10\n",
    "random_seed = 42\n",
    "\n",
    "# Create output directory structure\n",
    "for split in SPLITS:\n",
    "    for sub in [\"images\", \"masks\", \"distance_maps\", \"label_masks\", \"vague_masks\"]:\n",
    "        (OUT_ROOT / split / sub).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directory scaffold created under\", OUT_ROOT)\n",
    "\n",
    "# ================================\n",
    "# COLLECT VALID TUPLES\n",
    "# ================================\n",
    "\n",
    "data_tuples = []\n",
    "for organ in RAW_ROOT.iterdir():\n",
    "    if not organ.is_dir():\n",
    "        continue\n",
    "\n",
    "    tissue_dir = organ / \"tissue images\"\n",
    "    mask_dir   = organ / \"mask binary\"\n",
    "    dist_dir   = organ / \"distance maps\"\n",
    "    label_dir  = organ / \"label masks modify\"\n",
    "    vague_dir  = organ / \"vague areas\" / \"mask binary\"\n",
    "\n",
    "    for d in [tissue_dir, mask_dir, dist_dir, label_dir, vague_dir]:\n",
    "        if not d.exists():\n",
    "            print(f\"‚ö†Ô∏è Skipping {organ.name}: missing {d}\")\n",
    "            break\n",
    "    else:\n",
    "        for img_path in tissue_dir.glob(\"*.png\"):\n",
    "            stem = img_path.stem\n",
    "            m1 = mask_dir / f\"{stem}.png\"\n",
    "            m2 = dist_dir / f\"{stem}.png\"\n",
    "            m3 = label_dir / f\"{stem}.tif\"\n",
    "            m4 = vague_dir / f\"{stem}.png\"\n",
    "            if m1.exists() and m2.exists() and m3.exists() and m4.exists():\n",
    "                data_tuples.append((img_path, m1, m2, m3, m4))\n",
    "\n",
    "print(f\"‚úÖ Found {len(data_tuples)} complete data tuples.\")\n",
    "\n",
    "# ================================\n",
    "# SPLIT DATA\n",
    "# ================================\n",
    "\n",
    "train_val, test = train_test_split(data_tuples, test_size=test_frac, random_state=random_seed)\n",
    "train, val = train_test_split(train_val, test_size=val_frac / (1 - test_frac), random_state=random_seed)\n",
    "print(f\"üìä Split sizes ‚Äî Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "\n",
    "# ================================\n",
    "# PROCESS & SAVE RESIZED OUTPUT\n",
    "# ================================\n",
    "\n",
    "def process_and_save(split_list, split_name):\n",
    "    for (img, msk, dist, lbl, vmask) in split_list:\n",
    "        # Load all\n",
    "        arr_img  = cv2.imread(str(img))\n",
    "        arr_msk  = cv2.imread(str(msk),  cv2.IMREAD_GRAYSCALE)\n",
    "        arr_dist = cv2.imread(str(dist), cv2.IMREAD_GRAYSCALE)\n",
    "        arr_lbl  = cv2.imread(str(lbl),  cv2.IMREAD_UNCHANGED)\n",
    "        arr_v    = cv2.imread(str(vmask), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize all\n",
    "        img_r  = cv2.resize(arr_img,  TARGET_SIZE, interpolation=cv2.INTER_CUBIC)\n",
    "        msk_r  = cv2.resize(arr_msk,  TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "        dist_r = cv2.resize(arr_dist, TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "        lbl_r  = cv2.resize(arr_lbl,  TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "        v_r    = cv2.resize(arr_v,    TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Save\n",
    "        cv2.imwrite(str(OUT_ROOT/split_name/\"images\"/ img.name),  img_r)\n",
    "        cv2.imwrite(str(OUT_ROOT/split_name/\"masks\"/  msk.name),  msk_r)\n",
    "        cv2.imwrite(str(OUT_ROOT/split_name/\"distance_maps\"/ dist.name), dist_r)\n",
    "        cv2.imwrite(str(OUT_ROOT/split_name/\"label_masks\"/ lbl.name), lbl_r)\n",
    "        cv2.imwrite(str(OUT_ROOT/split_name/\"vague_masks\"/ vmask.name),  v_r)\n",
    "\n",
    "    print(f\"‚úÖ {split_name} saved: {len(split_list)} samples\")\n",
    "\n",
    "process_and_save(train, \"train\")\n",
    "process_and_save(val,   \"val\")\n",
    "process_and_save(test,  \"test\")\n",
    "\n",
    "# Confirm counts\n",
    "for split in SPLITS:\n",
    "    counts = {sub: len(list((OUT_ROOT/split/sub).glob(\"*.*\"))) for sub in [\"images\", \"masks\", \"distance_maps\", \"label_masks\", \"vague_masks\"]}\n",
    "    print(f\"{split} counts: \", counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##done in safe_downsample.py\n",
    "## Run using terminal \"python safe_downsample.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Super Resolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIG ==========\n",
    "downsampled_folder = \"downsampled_data/train/images\"\n",
    "original_folder = \"data/train/images\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths to models\n",
    "model_paths = {\n",
    "    \"srcnn\": \"models/srcnn_model.pth\",\n",
    "    \"espcn\": \"models/espcn_x2_256to512.pth\",\n",
    "    \"edsr_me\": \"models/edsr_x2_256to512.pth\",\n",
    "    \"edsr\": \"models/EDSR_x2.pb\"\n",
    "}\n",
    "\n",
    "# Output dictionary to store results\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SRCNN\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Define SRDFN\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return out + residual  # Skip connection\n",
    "\n",
    "class EDSR(nn.Module):\n",
    "    def __init__(self, scale_factor=2, num_channels=64, num_blocks=8):\n",
    "        super(EDSR, self).__init__()\n",
    "        self.entry = nn.Conv2d(3, num_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_channels) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 3 * (scale_factor ** 2), kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(scale_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry(x)\n",
    "        res = self.res_blocks(x)\n",
    "        x = self.conv(res) + x  # Global skip connection\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Define ESPCN\n",
    "class ESPCN(nn.Module):\n",
    "    def __init__(self, scale_factor=3):\n",
    "        super(ESPCN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 3 * (scale_factor ** 2), kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sr(model_name, img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Directly load whole model object\n",
    "    model = torch.load(model_paths[model_name], map_location=device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sr = model(input_tensor)\n",
    "\n",
    "    sr_img = sr.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    return np.clip(sr_img, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing SRCNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/531 [00:00<?, ?it/s]C:\\Users\\rita\\AppData\\Local\\Temp\\ipykernel_17528\\1356720203.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_paths[model_name], map_location=device)\n",
      "  0%|          | 0/531 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/srcnn_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m hr_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(hr_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Apply super-resolution\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m sr_img \u001b[38;5;241m=\u001b[39m \u001b[43mapply_sr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Resize to match HR image\u001b[39;00m\n\u001b[0;32m     25\u001b[0m sr_img_resized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mfromarray((sr_img \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\u001b[38;5;241m.\u001b[39mresize(hr_img\u001b[38;5;241m.\u001b[39msize))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mapply_sr\u001b[1;34m(model_name, img_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Directly load whole model object\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\rita\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\rita\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\rita\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/srcnn_model.pth'"
     ]
    }
   ],
   "source": [
    "model_names = [\"srcnn\", \"espcn\", \"edsr_me\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "\n",
    "    print(f\"üîç Testing {model_name.upper()}...\")\n",
    "\n",
    "    for filename in tqdm(os.listdir(downsampled_folder)):\n",
    "        lr_path = os.path.join(downsampled_folder, filename)\n",
    "        hr_path = os.path.join(original_folder, filename)\n",
    "\n",
    "        if not os.path.exists(hr_path):\n",
    "            continue\n",
    "\n",
    "        # Load images\n",
    "        lr_img = Image.open(lr_path).convert(\"RGB\")\n",
    "        hr_img = Image.open(hr_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply super-resolution\n",
    "        \n",
    "        sr_img = apply_sr(model_name, lr_path)\n",
    "\n",
    "        # Resize to match HR image\n",
    "        sr_img_resized = np.array(Image.fromarray((sr_img * 255).astype(np.uint8)).resize(hr_img.size)).astype(np.float32) / 255.0\n",
    "        hr_np = np.array(hr_img).astype(np.float32) / 255.0\n",
    "\n",
    "        # Metrics\n",
    "        psnr = peak_signal_noise_ratio(hr_np, sr_img_resized, data_range=1.0)\n",
    "        ssim = structural_similarity(hr_np, sr_img_resized, channel_axis=-1, data_range=1.0)\n",
    "        psnrs.append(psnr)\n",
    "        ssims.append(ssim)\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"PSNR\": np.mean(psnrs),\n",
    "        \"SSIM\": np.mean(ssims)\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ {model_name.upper()} ‚Äî PSNR: {np.mean(psnrs):.2f} | SSIM: {np.mean(ssims):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Comparison:\n",
      "SRCNN      | PSNR: 33.04 | SSIM: 0.7932\n",
      "ESPCN      | PSNR: 35.74 | SSIM: 0.8519\n",
      "EDSR_ME    | PSNR: 36.25 | SSIM: 0.8673\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Final Comparison:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name.upper():<10} | PSNR: {metrics['PSNR']:.2f} | SSIM: {metrics['SSIM']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
