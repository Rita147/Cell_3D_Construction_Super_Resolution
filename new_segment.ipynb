{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ca2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 0: Downgrade NumPy in the notebook environment and force a restart\n",
    "\n",
    "# Use the IPython “magic” so it installs into this kernel\n",
    "#%pip install --upgrade \"numpy<2.0\"\n",
    "\n",
    "# Then immediately exit so Jupyter will prompt you to restart\n",
    "#import os\n",
    "#os._exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb02feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 0: Install missing dependency\n",
    "#%pip install albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0847dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\albumentations\\check_version.py:147: UserWarning: Error fetching version info <urlopen error [Errno 11001] getaddrinfo failed>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 1: Imports & Hyperparameters (TensorFlow Keras only)\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint\n",
    "#from tensorflow.keras.layers import *\n",
    "#from tensorflow.keras.models import Model, load_model\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from albumentations import *\n",
    "#from tensorflow.keras import backend as K\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import watershed\n",
    "import skimage.morphology\n",
    "from skimage.io import imsave\n",
    "from skimage.morphology import remove_small_objects\n",
    "import tqdm\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "opts = {\n",
    "    'number_of_channel':       3,\n",
    "    'treshold':                0.5,\n",
    "    'epoch_num':               30,\n",
    "    'quick_run':               1,\n",
    "    'batch_size':              16,\n",
    "    'random_seed_num':         19,\n",
    "    'crop_size':               256,\n",
    "    'init_LR':                 0.001,\n",
    "    'LR_decay_factor':         0.5,\n",
    "    'LR_drop_after_nth_epoch': 20,\n",
    "    'result_save_path':        'prediction_image/',\n",
    "    'model_save_path':         'output_model/'\n",
    "}\n",
    "\n",
    "# Ensure output dirs exist\n",
    "os.makedirs(opts['model_save_path'], exist_ok=True)\n",
    "os.makedirs(opts['result_save_path'] + 'validation/unet',         exist_ok=True)\n",
    "os.makedirs(opts['result_save_path'] + 'validation/watershed_unet', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fa1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Data splits → globs\n",
    "# TRAIN\n",
    "train_img   = sorted(glob('data_sr_x2/train/images/*.png'))\n",
    "train_mask  = sorted(glob('data_sr_x2/train/masks/*.png'))\n",
    "train_dist  = sorted(glob('data_sr_x2/train/distance_maps/*.png'))\n",
    "train_label = sorted(glob('data_sr_x2/train/label_masks/*.tif'))\n",
    "train_vague = sorted(glob('data_sr_x2/train/vague_masks/*.png'))\n",
    "\n",
    "# VAL\n",
    "val_img     = sorted(glob('data_sr_x2/val/images/*.png'))\n",
    "val_mask    = sorted(glob('data_sr_x2/val/masks/*.png'))\n",
    "val_dist    = sorted(glob('data_sr_x2/val/distance_maps/*.png'))\n",
    "val_label   = sorted(glob('data_sr_x2/val/label_masks/*.tif'))\n",
    "val_vague   = sorted(glob('data_sr_x2/val/vague_masks/*.png'))\n",
    "\n",
    "# TEST\n",
    "test_img    = sorted(glob('data_sr_x2/test/images/*.png'))\n",
    "test_mask   = sorted(glob('data_sr_x2/test/masks/*.png'))\n",
    "test_dist   = sorted(glob('data_sr_x2/test/distance_maps/*.png'))\n",
    "test_label  = sorted(glob('data_sr_x2/test/label_masks/*.tif'))\n",
    "test_vague  = sorted(glob('data_sr_x2/test/vague_masks/*.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a41a257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Losses & Scheduler\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    inter = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * inter + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return 0.5 * tf.keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n",
    "\n",
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, epochs_drop=1000):\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/epochs_drop))\n",
    "    return tf.keras.callbacks.LearningRateScheduler(schedule, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f141b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: U-Net model definitions\n",
    "def deep_unet(IMG_CHANNELS, LearnRate):\n",
    "    inputs = tf.keras.layers.Input((None, None, IMG_CHANNELS))\n",
    "    # down\n",
    "    c1 = tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D()(c1)\n",
    "    c2 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D()(c2)\n",
    "    c3 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.1)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D()(c3)\n",
    "    c4 = tf.keras.layers.Conv2D(128,3, activation='relu', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(0.1)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128,3, activation='relu', padding='same')(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D()(c4)\n",
    "    # bottleneck\n",
    "    c5 = tf.keras.layers.Conv2D(256,3, activation='relu', padding='same')(p4)\n",
    "    c5 = tf.keras.layers.Dropout(0.1)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(256,3, activation='relu', padding='same')(c5)\n",
    "    # up\n",
    "    u6 = tf.keras.layers.Conv2DTranspose(128,2, strides=2, padding='same')(c5)\n",
    "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "    c6 = tf.keras.layers.Conv2D(128,3, activation='relu', padding='same')(u6)\n",
    "    c6 = tf.keras.layers.Dropout(0.1)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(128,3, activation='relu', padding='same')(c6)\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(64,2, strides=2, padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    c7 = tf.keras.layers.Conv2D(64,3, activation='relu', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(64,3, activation='relu', padding='same')(c7)\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(32,2, strides=2, padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    c8 = tf.keras.layers.Conv2D(32,3, activation='relu', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(32,3, activation='relu', padding='same')(c8)\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(16,2, strides=2, padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1])\n",
    "    c9 = tf.keras.layers.Conv2D(16,3, activation='relu', padding='same')(u9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(16,3, activation='relu', padding='same')(c9)\n",
    "    outputs = tf.keras.layers.Conv2D(1,1, activation='sigmoid')(c9)\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=LearnRate),    # ← use Adam, not adam_v2\n",
    "            loss=bce_dice_loss,\n",
    "            metrics=[dice_coef]\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e4ce973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize   import linear_sum_assignment\n",
    "import scipy           # for scipy.spatial.distance\n",
    "# %% Cell 5 (update): Augmentation function without invalid always_apply\n",
    "def albumentation_aug(p=1.0):\n",
    "    return Compose([\n",
    "        CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=0.15,\n",
    "                                 contrast_limit=0.15,\n",
    "                                 brightness_by_max=True,\n",
    "                                 p=0.4),\n",
    "    ], p=p)\n",
    "\n",
    "\n",
    "def get_fast_aji(true, pred):\n",
    "    \"\"\"AJI version distributed by MoNuSeg, has no permutation problem but suffered from \n",
    "    over-penalisation similar to DICE2.\n",
    "    Fast computation requires instance IDs are in contiguous orderding i.e [1, 2, 3, 4] \n",
    "    not [2, 3, 6, 10]. Please call `remap_label` before hand and `by_size` flag has no \n",
    "    effect on the result.\n",
    "    \"\"\"\n",
    "    true = np.copy(true)  # ? do we need this\n",
    "    pred = np.copy(pred)\n",
    "    true_id_list = list(np.unique(true))\n",
    "    pred_id_list = list(np.unique(pred))\n",
    "    #print(len(pred_id_list))\n",
    "    if len(pred_id_list) == 1:\n",
    "        return 0\n",
    "\n",
    "    true_masks = [None,]\n",
    "    for t in true_id_list[1:]:\n",
    "        t_mask = np.array(true == t, np.uint8)\n",
    "        true_masks.append(t_mask)\n",
    "\n",
    "    pred_masks = [None,]\n",
    "    for p in pred_id_list[1:]:\n",
    "        p_mask = np.array(pred == p, np.uint8)\n",
    "        pred_masks.append(p_mask)\n",
    "\n",
    "    # prefill with value\n",
    "    pairwise_inter = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "    pairwise_union = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "\n",
    "    # caching pairwise\n",
    "    for true_id in true_id_list[1:]:  # 0-th is background\n",
    "        t_mask = true_masks[true_id]\n",
    "        pred_true_overlap = pred[t_mask > 0]\n",
    "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
    "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
    "        for pred_id in pred_true_overlap_id:\n",
    "            if pred_id == 0:  # ignore\n",
    "                continue  # overlaping background\n",
    "            p_mask = pred_masks[pred_id]\n",
    "            total = (t_mask + p_mask).sum()\n",
    "            inter = (t_mask * p_mask).sum()\n",
    "            pairwise_inter[true_id - 1, pred_id - 1] = inter\n",
    "            pairwise_union[true_id - 1, pred_id - 1] = total - inter\n",
    "\n",
    "    pairwise_iou = pairwise_inter / (pairwise_union + 1.0e-6)\n",
    "    # pair of pred that give highest iou for each true, dont care\n",
    "    # about reusing pred instance multiple times\n",
    "    paired_pred = np.argmax(pairwise_iou, axis=1)\n",
    "    pairwise_iou = np.max(pairwise_iou, axis=1)\n",
    "    # exlude those dont have intersection\n",
    "    paired_true = np.nonzero(pairwise_iou > 0.0)[0]\n",
    "    paired_pred = paired_pred[paired_true]\n",
    "    # print(paired_true.shape, paired_pred.shape)\n",
    "    overall_inter = (pairwise_inter[paired_true, paired_pred]).sum()\n",
    "    overall_union = (pairwise_union[paired_true, paired_pred]).sum()\n",
    "\n",
    "    paired_true = list(paired_true + 1)  # index to instance ID\n",
    "    paired_pred = list(paired_pred + 1)\n",
    "    # add all unpaired GT and Prediction into the union\n",
    "    unpaired_true = np.array(\n",
    "        [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
    "    )\n",
    "    unpaired_pred = np.array(\n",
    "        [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
    "    )\n",
    "    for true_id in unpaired_true:\n",
    "        overall_union += true_masks[true_id].sum()\n",
    "    for pred_id in unpaired_pred:\n",
    "        overall_union += pred_masks[pred_id].sum()\n",
    "\n",
    "    aji_score = overall_inter / overall_union\n",
    "    #print(aji_score)\n",
    "    return aji_score\n",
    "\n",
    "def get_fast_pq(true, pred, match_iou=0.5):\n",
    "    \"\"\"`match_iou` is the IoU threshold level to determine the pairing between\n",
    "    GT instances `p` and prediction instances `g`. `p` and `g` is a pair\n",
    "    if IoU > `match_iou`. However, pair of `p` and `g` must be unique \n",
    "    (1 prediction instance to 1 GT instance mapping).\n",
    "    If `match_iou` < 0.5, Munkres assignment (solving minimum weight matching\n",
    "    in bipartite graphs) is caculated to find the maximal amount of unique pairing. \n",
    "    If `match_iou` >= 0.5, all IoU(p,g) > 0.5 pairing is proven to be unique and\n",
    "    the number of pairs is also maximal.    \n",
    "    \n",
    "    Fast computation requires instance IDs are in contiguous orderding \n",
    "    i.e [1, 2, 3, 4] not [2, 3, 6, 10]. Please call `remap_label` beforehand \n",
    "    and `by_size` flag has no effect on the result.\n",
    "    Returns:\n",
    "        [dq, sq, pq]: measurement statistic\n",
    "        [paired_true, paired_pred, unpaired_true, unpaired_pred]: \n",
    "                      pairing information to perform measurement\n",
    "                    \n",
    "    \"\"\"\n",
    "    assert match_iou >= 0.0, \"Cant' be negative\"\n",
    "\n",
    "    true = np.copy(true)\n",
    "    pred = np.copy(pred)\n",
    "    true_id_list = list(np.unique(true))\n",
    "    pred_id_list = list(np.unique(pred))\n",
    "    \n",
    "    if len(pred_id_list) == 1:\n",
    "        return [0, 0, 0], [0,0, 0, 0]\n",
    "\n",
    "    true_masks = [\n",
    "        None,\n",
    "    ]\n",
    "    for t in true_id_list[1:]:\n",
    "        t_mask = np.array(true == t, np.uint8)\n",
    "        true_masks.append(t_mask)\n",
    "\n",
    "    pred_masks = [\n",
    "        None,\n",
    "    ]\n",
    "    for p in pred_id_list[1:]:\n",
    "        p_mask = np.array(pred == p, np.uint8)\n",
    "        pred_masks.append(p_mask)\n",
    "\n",
    "    # prefill with value\n",
    "    pairwise_iou = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "\n",
    "    # caching pairwise iou\n",
    "    for true_id in true_id_list[1:]:  # 0-th is background\n",
    "        t_mask = true_masks[true_id]\n",
    "        pred_true_overlap = pred[t_mask > 0]\n",
    "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
    "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
    "        for pred_id in pred_true_overlap_id:\n",
    "            if pred_id == 0:  # ignore\n",
    "                continue  # overlaping background\n",
    "            p_mask = pred_masks[pred_id]\n",
    "            total = (t_mask + p_mask).sum()\n",
    "            inter = (t_mask * p_mask).sum()\n",
    "            iou = inter / (total - inter)\n",
    "            pairwise_iou[true_id - 1, pred_id - 1] = iou\n",
    "    #\n",
    "    if match_iou >= 0.5:\n",
    "        paired_iou = pairwise_iou[pairwise_iou > match_iou]\n",
    "        pairwise_iou[pairwise_iou <= match_iou] = 0.0\n",
    "        paired_true, paired_pred = np.nonzero(pairwise_iou)\n",
    "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
    "        paired_true += 1  # index is instance id - 1\n",
    "        paired_pred += 1  # hence return back to original\n",
    "    else:  # * Exhaustive maximal unique pairing\n",
    "        #### Munkres pairing with scipy library\n",
    "        # the algorithm return (row indices, matched column indices)\n",
    "        # if there is multiple same cost in a row, index of first occurence\n",
    "        # is return, thus the unique pairing is ensure\n",
    "        # inverse pair to get high IoU as minimum\n",
    "        paired_true, paired_pred = linear_sum_assignment(-pairwise_iou)\n",
    "        ### extract the paired cost and remove invalid pair\n",
    "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
    "\n",
    "        # now select those above threshold level\n",
    "        # paired with iou = 0.0 i.e no intersection => FP or FN\n",
    "        paired_true = list(paired_true[paired_iou > match_iou] + 1)\n",
    "        paired_pred = list(paired_pred[paired_iou > match_iou] + 1)\n",
    "        paired_iou = paired_iou[paired_iou > match_iou]\n",
    "\n",
    "    # get the actual FP and FN\n",
    "    unpaired_true = [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
    "    unpaired_pred = [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
    "    # print(paired_iou.shape, paired_true.shape, len(unpaired_true), len(unpaired_pred))\n",
    "\n",
    "    #\n",
    "    tp = len(paired_true)\n",
    "    fp = len(unpaired_pred)\n",
    "    fn = len(unpaired_true)\n",
    "    # get the F1-score i.e DQ\n",
    "    dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n",
    "    # get the SQ, no paired has 0 iou so not impact\n",
    "    sq = paired_iou.sum() / (tp + 1.0e-6)\n",
    "\n",
    "    return [dq, sq, dq * sq], [paired_true, paired_pred, unpaired_true, unpaired_pred]\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "def get_dice_1(true, pred):\n",
    "    \"\"\"Traditional dice.\"\"\"\n",
    "    # cast to binary 1st\n",
    "    true = np.copy(true)\n",
    "    pred = np.copy(pred)\n",
    "    true[true > 0] = 1\n",
    "    pred[pred > 0] = 1\n",
    "    inter = true * pred\n",
    "    denom = true + pred\n",
    "    dice_score = 2.0 * np.sum(inter) / (np.sum(denom) + 0.0001)\n",
    "    if np.sum(inter)==0 and np.sum(denom)==0:\n",
    "        dice_score = 1 # to handel cases without any nuclei\n",
    "    #print(dice_score)\n",
    "    return dice_score\n",
    "\n",
    "#############################################################################################################\n",
    "def remap_label(pred, by_size=False):\n",
    "    \"\"\"Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3] \n",
    "    not [0, 2, 4, 6]. The ordering of instances (which one comes first) \n",
    "    is preserved unless by_size=True, then the instances will be reordered\n",
    "    so that bigger nucler has smaller ID.\n",
    "    Args:\n",
    "        pred    : the 2d array contain instances where each instances is marked\n",
    "                  by non-zero integer\n",
    "        by_size : renaming with larger nuclei has smaller id (on-top)\n",
    "    \"\"\"\n",
    "    pred_id = list(np.unique(pred))\n",
    "    pred_id.remove(0)\n",
    "    if len(pred_id) == 0:\n",
    "        return pred  # no label\n",
    "    if by_size:\n",
    "        pred_size = []\n",
    "        for inst_id in pred_id:\n",
    "            size = (pred == inst_id).sum()\n",
    "            pred_size.append(size)\n",
    "        # sort the id by size in descending order\n",
    "        pair_list = zip(pred_id, pred_size)\n",
    "        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n",
    "        pred_id, pred_size = zip(*pair_list)\n",
    "\n",
    "    new_pred = np.zeros(pred.shape, np.int32)\n",
    "    for idx, inst_id in enumerate(pred_id):\n",
    "        new_pred[pred == inst_id] = idx + 1\n",
    "    return new_pred\n",
    "\n",
    "def get_id_from_file_path(fp, indicator):\n",
    "    return os.path.basename(fp).replace(indicator, '')\n",
    "\n",
    "def chunker(seq, seq2, size):\n",
    "    return ([seq[i:i+size], seq2[i:i+size]] for i in range(0, len(seq), size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294950f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Data Generator (fixed to yield float32 masks)\n",
    "def data_gen(list_files, list_masks, batch_size, p,\n",
    "            augment=False):\n",
    "    aug = albumentation_aug(p)\n",
    "    while True:\n",
    "        for batch_imgs, batch_msks in chunker(list_files, list_masks, batch_size):\n",
    "            X, Y = [], []\n",
    "            for img_p, m_p in zip(batch_imgs, batch_msks):\n",
    "                x = cv2.imread(img_p)\n",
    "                x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "                m = cv2.imread(m_p, cv2.IMREAD_GRAYSCALE)\n",
    "                # binarize mask (0 or 1)\n",
    "                m_bin = (m == 255).astype(np.uint8)\n",
    "\n",
    "                if augment:\n",
    "                    augd = aug(image=x, mask=m_bin)\n",
    "                    x, m_bin = augd['image'], augd['mask']\n",
    "\n",
    "                X.append(x / 255.0)\n",
    "                Y.append(m_bin)\n",
    "\n",
    "            # convert to float32 so loss/metrics see float32 * float32\n",
    "            X = np.array(X, dtype=np.float32)\n",
    "            Y = np.expand_dims(np.array(Y, dtype=np.float32), -1)\n",
    "\n",
    "            yield X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef54452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.2239 - loss: 0.0370 \n",
      "Epoch 1: val_dice_coef improved from -inf to 0.25334, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 15s/step - dice_coef: 0.2240 - loss: 0.0354 - val_dice_coef: 0.2533 - val_loss: -0.0844 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.3226 - loss: -0.1040 \n",
      "Epoch 2: val_dice_coef improved from 0.25334 to 0.37702, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 14s/step - dice_coef: 0.3231 - loss: -0.1031 - val_dice_coef: 0.3770 - val_loss: -0.2134 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.3767 - loss: -0.1437 \n",
      "Epoch 3: val_dice_coef did not improve from 0.37702\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 14s/step - dice_coef: 0.3762 - loss: -0.1440 - val_dice_coef: 0.3213 - val_loss: -0.1921 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4135 - loss: -0.2164 \n",
      "Epoch 4: val_dice_coef did not improve from 0.37702\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 14s/step - dice_coef: 0.4128 - loss: -0.2156 - val_dice_coef: 0.3073 - val_loss: -0.2083 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4055 - loss: -0.1836 \n",
      "Epoch 5: val_dice_coef improved from 0.37702 to 0.38475, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 14s/step - dice_coef: 0.4058 - loss: -0.1844 - val_dice_coef: 0.3847 - val_loss: -0.2488 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4211 - loss: -0.2224 \n",
      "Epoch 6: val_dice_coef improved from 0.38475 to 0.47074, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 14s/step - dice_coef: 0.4218 - loss: -0.2231 - val_dice_coef: 0.4707 - val_loss: -0.3156 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4070 - loss: -0.1917 \n",
      "Epoch 7: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 14s/step - dice_coef: 0.4075 - loss: -0.1924 - val_dice_coef: 0.3995 - val_loss: -0.2201 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 8/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.3968 - loss: -0.2102 \n",
      "Epoch 8: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 14s/step - dice_coef: 0.3984 - loss: -0.2117 - val_dice_coef: 0.1231 - val_loss: 0.4916 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 9/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4036 - loss: -0.1777 \n",
      "Epoch 9: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 14s/step - dice_coef: 0.4043 - loss: -0.1785 - val_dice_coef: 0.3296 - val_loss: -0.2369 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 10/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.3612 - loss: -0.1780 \n",
      "Epoch 10: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 14s/step - dice_coef: 0.3632 - loss: -0.1793 - val_dice_coef: 0.3662 - val_loss: -0.2637 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 11/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.3426 - loss: -0.1607 \n",
      "Epoch 11: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 14s/step - dice_coef: 0.3453 - loss: -0.1626 - val_dice_coef: 0.4416 - val_loss: -0.2866 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 12/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4138 - loss: -0.2388 \n",
      "Epoch 12: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 14s/step - dice_coef: 0.4152 - loss: -0.2395 - val_dice_coef: 0.4607 - val_loss: -0.3376 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 13/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4332 - loss: -0.2491 \n",
      "Epoch 13: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 14s/step - dice_coef: 0.4338 - loss: -0.2492 - val_dice_coef: 0.3689 - val_loss: -0.2498 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 14/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4407 - loss: -0.2632 \n",
      "Epoch 14: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 14s/step - dice_coef: 0.4414 - loss: -0.2633 - val_dice_coef: 0.4336 - val_loss: -0.3157 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 15/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4752 - loss: -0.3011 \n",
      "Epoch 15: val_dice_coef did not improve from 0.47074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 14s/step - dice_coef: 0.4755 - loss: -0.3010 - val_dice_coef: 0.4205 - val_loss: -0.2883 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 16/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.5243 - loss: -0.3388 \n",
      "Epoch 16: val_dice_coef improved from 0.47074 to 0.48148, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 14s/step - dice_coef: 0.5242 - loss: -0.3385 - val_dice_coef: 0.4815 - val_loss: -0.3351 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 17/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.4978 - loss: -0.3025 \n",
      "Epoch 17: val_dice_coef improved from 0.48148 to 0.54618, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 14s/step - dice_coef: 0.4980 - loss: -0.3030 - val_dice_coef: 0.5462 - val_loss: -0.4068 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 18/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.5435 - loss: -0.3561 \n",
      "Epoch 18: val_dice_coef did not improve from 0.54618\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 14s/step - dice_coef: 0.5426 - loss: -0.3553 - val_dice_coef: 0.4361 - val_loss: -0.3395 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 19/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.5712 - loss: -0.4000 \n",
      "Epoch 19: val_dice_coef did not improve from 0.54618\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 14s/step - dice_coef: 0.5704 - loss: -0.3988 - val_dice_coef: 0.4778 - val_loss: -0.3883 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 20/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.5656 - loss: -0.4115 \n",
      "Epoch 20: val_dice_coef did not improve from 0.54618\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 14s/step - dice_coef: 0.5652 - loss: -0.4109 - val_dice_coef: 0.5154 - val_loss: -0.4375 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 21/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.5971 - loss: -0.4527 \n",
      "Epoch 21: val_dice_coef improved from 0.54618 to 0.60476, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 14s/step - dice_coef: 0.5969 - loss: -0.4521 - val_dice_coef: 0.6048 - val_loss: -0.4692 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 22/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.6228 - loss: -0.4731 \n",
      "Epoch 22: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 14s/step - dice_coef: 0.6220 - loss: -0.4722 - val_dice_coef: 0.5051 - val_loss: -0.4044 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 23/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.6121 - loss: -0.4511 \n",
      "Epoch 23: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 14s/step - dice_coef: 0.6118 - loss: -0.4511 - val_dice_coef: 0.4911 - val_loss: -0.3912 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 24/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.6394 - loss: -0.4854 \n",
      "Epoch 24: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 14s/step - dice_coef: 0.6389 - loss: -0.4853 - val_dice_coef: 0.5275 - val_loss: -0.4575 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 25/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.6411 - loss: -0.4797 \n",
      "Epoch 25: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 14s/step - dice_coef: 0.6407 - loss: -0.4797 - val_dice_coef: 0.5291 - val_loss: -0.4327 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 26/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.6564 - loss: -0.5072 \n",
      "Epoch 26: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 14s/step - dice_coef: 0.6557 - loss: -0.5069 - val_dice_coef: 0.5803 - val_loss: -0.4233 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 27/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - dice_coef: 0.6642 - loss: -0.5061 \n",
      "Epoch 27: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 14s/step - dice_coef: 0.6635 - loss: -0.5061 - val_dice_coef: 0.6039 - val_loss: -0.4969 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 28/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.6720 - loss: -0.5163 \n",
      "Epoch 28: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 14s/step - dice_coef: 0.6711 - loss: -0.5158 - val_dice_coef: 0.5289 - val_loss: -0.4449 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 29/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.6636 - loss: -0.5146 \n",
      "Epoch 29: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 14s/step - dice_coef: 0.6629 - loss: -0.5142 - val_dice_coef: 0.5391 - val_loss: -0.4558 - learning_rate: 5.0000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 30/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - dice_coef: 0.6608 - loss: -0.5118 \n",
      "Epoch 30: val_dice_coef did not improve from 0.60476\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 14s/step - dice_coef: 0.6601 - loss: -0.5113 - val_dice_coef: 0.5424 - val_loss: -0.4352 - learning_rate: 5.0000e-04\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 446ms/step\n",
      "Test Dice: 0.5715 ± 0.2265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Cell 7: Train → Validation → Test Evaluation\n",
    "# callbacks\n",
    "logger     = tf.keras.callbacks.CSVLogger(opts['model_save_path'] + 'unet.log')\n",
    "lr_drop    = step_decay_schedule(\n",
    "                initial_lr   = opts['init_LR'],\n",
    "                decay_factor = opts['LR_decay_factor'],\n",
    "                epochs_drop  = opts['LR_drop_after_nth_epoch']\n",
    "            )\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    opts['model_save_path'] + 'unet.weights.h5',   # ← note the new extension\n",
    "    monitor='val_dice_coef',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# build model\n",
    "model = deep_unet(opts['number_of_channel'], opts['init_LR'])\n",
    "\n",
    "# train\n",
    "history = model.fit(\n",
    "    data_gen(train_img, train_mask, opts['batch_size'], 1,\n",
    "              augment=True),\n",
    "    validation_data = data_gen(val_img, val_mask, opts['batch_size'], 1,\n",
    "                            augment=False),\n",
    "    steps_per_epoch   = len(train_img)//opts['batch_size'],\n",
    "    validation_steps  = max(1, len(val_img)//opts['batch_size']),\n",
    "    epochs            = opts['epoch_num'],\n",
    "    callbacks         = [checkpoint, logger, lr_drop],\n",
    "    verbose           = 1\n",
    ")\n",
    "\n",
    "# load best\n",
    "model.load_weights(opts['model_save_path'] + 'unet.weights.h5')\n",
    "\n",
    "# evaluate on test\n",
    "preds = model.predict(\n",
    "    data_gen(test_img, test_mask, batch_size=1, p=1, augment=False),\n",
    "    steps=len(test_img)\n",
    ")\n",
    "preds_bin = (preds>opts['treshold']).astype('uint8')\n",
    "\n",
    "dice_scores = []\n",
    "for i, gt_p in enumerate(test_mask):\n",
    "    gt = cv2.imread(gt_p, cv2.IMREAD_GRAYSCALE)\n",
    "    pr = cv2.resize(preds_bin[i,...,0], (gt.shape[1], gt.shape[0]),\n",
    "                    interpolation=cv2.INTER_NEAREST)\n",
    "    dice_scores.append(get_dice_1(gt, pr))\n",
    "\n",
    "print(f\"Test Dice: {np.mean(dice_scores):.4f} ± {np.std(dice_scores):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
