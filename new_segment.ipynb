{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ca2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 0: Downgrade NumPy in the notebook environment and force a restart\n",
    "\n",
    "# Use the IPython “magic” so it installs into this kernel\n",
    "#%pip install --upgrade \"numpy<2.0\"\n",
    "\n",
    "# Then immediately exit so Jupyter will prompt you to restart\n",
    "#import os\n",
    "#os._exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb02feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 0: Install missing dependency\n",
    "#%pip install albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Imports & Hyperparameters (TensorFlow Keras only)\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint\n",
    "#from tensorflow.keras.layers import *\n",
    "#from tensorflow.keras.models import Model, load_model\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from albumentations import *\n",
    "#from tensorflow.keras import backend as K\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import watershed\n",
    "import skimage.morphology\n",
    "from skimage.io import imsave\n",
    "from skimage.morphology import remove_small_objects\n",
    "import tqdm\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "opts = {\n",
    "    'number_of_channel':       3,\n",
    "    'treshold':                0.5,\n",
    "    'epoch_num':               50,\n",
    "    'quick_run':               1,\n",
    "    'batch_size':              16,\n",
    "    'random_seed_num':         19,\n",
    "    'crop_size':               256,\n",
    "    'init_LR':                 0.001,\n",
    "    'LR_decay_factor':         0.5,\n",
    "    'LR_drop_after_nth_epoch': 20,\n",
    "    'result_save_path':        'prediction_image/',\n",
    "    'model_save_path':         'output_model/'\n",
    "}\n",
    "\n",
    "# Ensure output dirs exist\n",
    "os.makedirs(opts['model_save_path'], exist_ok=True)\n",
    "os.makedirs(opts['result_save_path'] + 'validation/unet',         exist_ok=True)\n",
    "os.makedirs(opts['result_save_path'] + 'validation/watershed_unet', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37fa1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Data splits → globs\n",
    "# TRAIN\n",
    "train_img   = sorted(glob('data_sr_x2/train/images/*.png'))\n",
    "train_mask  = sorted(glob('data_sr_x2/train/masks/*.png'))\n",
    "train_dist  = sorted(glob('data_sr_x2/train/distance_maps/*.png'))\n",
    "train_label = sorted(glob('data_sr_x2/train/label_masks/*.tif'))\n",
    "train_vague = sorted(glob('data_sr_x2/train/vague_masks/*.png'))\n",
    "\n",
    "# VAL\n",
    "val_img     = sorted(glob('data_sr_x2/val/images/*.png'))\n",
    "val_mask    = sorted(glob('data_sr_x2/val/masks/*.png'))\n",
    "val_dist    = sorted(glob('data_sr_x2/val/distance_maps/*.png'))\n",
    "val_label   = sorted(glob('data_sr_x2/val/label_masks/*.tif'))\n",
    "val_vague   = sorted(glob('data_sr_x2/val/vague_masks/*.png'))\n",
    "\n",
    "# TEST\n",
    "test_img    = sorted(glob('data_sr_x2/test/images/*.png'))\n",
    "test_mask   = sorted(glob('data_sr_x2/test/masks/*.png'))\n",
    "test_dist   = sorted(glob('data_sr_x2/test/distance_maps/*.png'))\n",
    "test_label  = sorted(glob('data_sr_x2/test/label_masks/*.tif'))\n",
    "test_vague  = sorted(glob('data_sr_x2/test/vague_masks/*.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41a257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Losses & Scheduler\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    inter = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * inter + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return 0.5 * tf.keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n",
    "\n",
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, epochs_drop=1000):\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/epochs_drop))\n",
    "    return tf.keras.callbacks.LearningRateScheduler(schedule, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f141b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: U-Net model definitions (true “deep” version)\n",
    "def deep_unet(IMG_CHANNELS, LearnRate):\n",
    "    inputs = tf.keras.layers.Input((None, None, IMG_CHANNELS))\n",
    "\n",
    "    # ── Down 1: 16\n",
    "    c1 = tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D()(c1)\n",
    "\n",
    "    # ── Down 2: 32\n",
    "    c2 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D()(c2)\n",
    "\n",
    "    # ── Down 3: 64\n",
    "    c3 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.1)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D()(c3)\n",
    "\n",
    "    # ── Down 4: 128\n",
    "    c4 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(0.1)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D()(c4)\n",
    "\n",
    "    # ── Down 5 (extra block): 256\n",
    "    c5 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p4)\n",
    "    c5 = tf.keras.layers.Dropout(0.1)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c5)\n",
    "    p5 = tf.keras.layers.MaxPooling2D()(c5)\n",
    "\n",
    "    # ── Bottleneck: 512\n",
    "    c6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p5)\n",
    "    c6 = tf.keras.layers.Dropout(0.1)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c6)\n",
    "\n",
    "    # ── Up 1: merge with c5\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(256, 2, strides=2, padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c5])\n",
    "    c7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c7)\n",
    "\n",
    "    # ── Up 2: merge with c4\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(128, 2, strides=2, padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c4])\n",
    "    c8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c8)\n",
    "\n",
    "    # ── Up 3: merge with c3\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(64, 2, strides=2, padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c3])\n",
    "    c9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c9)\n",
    "\n",
    "    # ── Up 4: merge with c2\n",
    "    u10 = tf.keras.layers.Conv2DTranspose(32, 2, strides=2, padding='same')(c9)\n",
    "    u10 = tf.keras.layers.concatenate([u10, c2])\n",
    "    c10 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u10)\n",
    "    c10 = tf.keras.layers.Dropout(0.1)(c10)\n",
    "    c10 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c10)\n",
    "\n",
    "    # ── Up 5: merge with c1\n",
    "    u11 = tf.keras.layers.Conv2DTranspose(16, 2, strides=2, padding='same')(c10)\n",
    "    u11 = tf.keras.layers.concatenate([u11, c1])\n",
    "    c11 = tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u11)\n",
    "    c11 = tf.keras.layers.Dropout(0.1)(c11)\n",
    "    c11 = tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c11)\n",
    "\n",
    "    # ── Output\n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(c11)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LearnRate),\n",
    "        loss=bce_dice_loss,\n",
    "        metrics=[dice_coef]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e4ce973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize   import linear_sum_assignment\n",
    "import scipy           # for scipy.spatial.distance\n",
    "# %% Cell 5: Augmentation & Metrics Helpers (updated)\n",
    "from albumentations import (\n",
    "    Compose, RandomCrop, CLAHE, RandomBrightnessContrast,\n",
    "    HueSaturationValue, HorizontalFlip, VerticalFlip,\n",
    "    RandomRotate90, ShiftScaleRotate\n",
    ")\n",
    "# %% Cell 5 (update): Augmentation function without invalid always_apply\n",
    "def albumentation_aug(p=1.0, crop_size_row=256, crop_size_col=256):\n",
    "    return Compose([\n",
    "        RandomCrop(crop_size_row, crop_size_col, p=1.0),\n",
    "        CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=0.15,\n",
    "                                 contrast_limit=0.15,\n",
    "                                 brightness_by_max=True,\n",
    "                                 p=0.4),\n",
    "        HueSaturationValue(hue_shift_limit=20,\n",
    "                           sat_shift_limit=20,\n",
    "                           val_shift_limit=20,\n",
    "                           p=0.1),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        RandomRotate90(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.0625,\n",
    "                         scale_limit=0.1,\n",
    "                         rotate_limit=20,\n",
    "                         interpolation=1,\n",
    "                         border_mode=4,\n",
    "                         p=0.1),\n",
    "    ], p=p)\n",
    "\n",
    "\n",
    "def get_fast_aji(true, pred):\n",
    "    \"\"\"AJI version distributed by MoNuSeg, has no permutation problem but suffered from \n",
    "    over-penalisation similar to DICE2.\n",
    "    Fast computation requires instance IDs are in contiguous orderding i.e [1, 2, 3, 4] \n",
    "    not [2, 3, 6, 10]. Please call `remap_label` before hand and `by_size` flag has no \n",
    "    effect on the result.\n",
    "    \"\"\"\n",
    "    true = np.copy(true)  # ? do we need this\n",
    "    pred = np.copy(pred)\n",
    "    true_id_list = list(np.unique(true))\n",
    "    pred_id_list = list(np.unique(pred))\n",
    "    #print(len(pred_id_list))\n",
    "    if len(pred_id_list) == 1:\n",
    "        return 0\n",
    "\n",
    "    true_masks = [None,]\n",
    "    for t in true_id_list[1:]:\n",
    "        t_mask = np.array(true == t, np.uint8)\n",
    "        true_masks.append(t_mask)\n",
    "\n",
    "    pred_masks = [None,]\n",
    "    for p in pred_id_list[1:]:\n",
    "        p_mask = np.array(pred == p, np.uint8)\n",
    "        pred_masks.append(p_mask)\n",
    "\n",
    "    # prefill with value\n",
    "    pairwise_inter = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "    pairwise_union = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "\n",
    "    # caching pairwise\n",
    "    for true_id in true_id_list[1:]:  # 0-th is background\n",
    "        t_mask = true_masks[true_id]\n",
    "        pred_true_overlap = pred[t_mask > 0]\n",
    "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
    "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
    "        for pred_id in pred_true_overlap_id:\n",
    "            if pred_id == 0:  # ignore\n",
    "                continue  # overlaping background\n",
    "            p_mask = pred_masks[pred_id]\n",
    "            total = (t_mask + p_mask).sum()\n",
    "            inter = (t_mask * p_mask).sum()\n",
    "            pairwise_inter[true_id - 1, pred_id - 1] = inter\n",
    "            pairwise_union[true_id - 1, pred_id - 1] = total - inter\n",
    "\n",
    "    pairwise_iou = pairwise_inter / (pairwise_union + 1.0e-6)\n",
    "    # pair of pred that give highest iou for each true, dont care\n",
    "    # about reusing pred instance multiple times\n",
    "    paired_pred = np.argmax(pairwise_iou, axis=1)\n",
    "    pairwise_iou = np.max(pairwise_iou, axis=1)\n",
    "    # exlude those dont have intersection\n",
    "    paired_true = np.nonzero(pairwise_iou > 0.0)[0]\n",
    "    paired_pred = paired_pred[paired_true]\n",
    "    # print(paired_true.shape, paired_pred.shape)\n",
    "    overall_inter = (pairwise_inter[paired_true, paired_pred]).sum()\n",
    "    overall_union = (pairwise_union[paired_true, paired_pred]).sum()\n",
    "\n",
    "    paired_true = list(paired_true + 1)  # index to instance ID\n",
    "    paired_pred = list(paired_pred + 1)\n",
    "    # add all unpaired GT and Prediction into the union\n",
    "    unpaired_true = np.array(\n",
    "        [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
    "    )\n",
    "    unpaired_pred = np.array(\n",
    "        [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
    "    )\n",
    "    for true_id in unpaired_true:\n",
    "        overall_union += true_masks[true_id].sum()\n",
    "    for pred_id in unpaired_pred:\n",
    "        overall_union += pred_masks[pred_id].sum()\n",
    "\n",
    "    aji_score = overall_inter / overall_union\n",
    "    #print(aji_score)\n",
    "    return aji_score\n",
    "\n",
    "def get_fast_pq(true, pred, match_iou=0.5):\n",
    "    \"\"\"`match_iou` is the IoU threshold level to determine the pairing between\n",
    "    GT instances `p` and prediction instances `g`. `p` and `g` is a pair\n",
    "    if IoU > `match_iou`. However, pair of `p` and `g` must be unique \n",
    "    (1 prediction instance to 1 GT instance mapping).\n",
    "    If `match_iou` < 0.5, Munkres assignment (solving minimum weight matching\n",
    "    in bipartite graphs) is caculated to find the maximal amount of unique pairing. \n",
    "    If `match_iou` >= 0.5, all IoU(p,g) > 0.5 pairing is proven to be unique and\n",
    "    the number of pairs is also maximal.    \n",
    "    \n",
    "    Fast computation requires instance IDs are in contiguous orderding \n",
    "    i.e [1, 2, 3, 4] not [2, 3, 6, 10]. Please call `remap_label` beforehand \n",
    "    and `by_size` flag has no effect on the result.\n",
    "    Returns:\n",
    "        [dq, sq, pq]: measurement statistic\n",
    "        [paired_true, paired_pred, unpaired_true, unpaired_pred]: \n",
    "                      pairing information to perform measurement\n",
    "                    \n",
    "    \"\"\"\n",
    "    assert match_iou >= 0.0, \"Cant' be negative\"\n",
    "\n",
    "    true = np.copy(true)\n",
    "    pred = np.copy(pred)\n",
    "    true_id_list = list(np.unique(true))\n",
    "    pred_id_list = list(np.unique(pred))\n",
    "    \n",
    "    if len(pred_id_list) == 1:\n",
    "        return [0, 0, 0], [0,0, 0, 0]\n",
    "\n",
    "    true_masks = [\n",
    "        None,\n",
    "    ]\n",
    "    for t in true_id_list[1:]:\n",
    "        t_mask = np.array(true == t, np.uint8)\n",
    "        true_masks.append(t_mask)\n",
    "\n",
    "    pred_masks = [\n",
    "        None,\n",
    "    ]\n",
    "    for p in pred_id_list[1:]:\n",
    "        p_mask = np.array(pred == p, np.uint8)\n",
    "        pred_masks.append(p_mask)\n",
    "\n",
    "    # prefill with value\n",
    "    pairwise_iou = np.zeros(\n",
    "        [len(true_id_list) - 1, len(pred_id_list) - 1], dtype=np.float64\n",
    "    )\n",
    "\n",
    "    # caching pairwise iou\n",
    "    for true_id in true_id_list[1:]:  # 0-th is background\n",
    "        t_mask = true_masks[true_id]\n",
    "        pred_true_overlap = pred[t_mask > 0]\n",
    "        pred_true_overlap_id = np.unique(pred_true_overlap)\n",
    "        pred_true_overlap_id = list(pred_true_overlap_id)\n",
    "        for pred_id in pred_true_overlap_id:\n",
    "            if pred_id == 0:  # ignore\n",
    "                continue  # overlaping background\n",
    "            p_mask = pred_masks[pred_id]\n",
    "            total = (t_mask + p_mask).sum()\n",
    "            inter = (t_mask * p_mask).sum()\n",
    "            iou = inter / (total - inter)\n",
    "            pairwise_iou[true_id - 1, pred_id - 1] = iou\n",
    "    #\n",
    "    if match_iou >= 0.5:\n",
    "        paired_iou = pairwise_iou[pairwise_iou > match_iou]\n",
    "        pairwise_iou[pairwise_iou <= match_iou] = 0.0\n",
    "        paired_true, paired_pred = np.nonzero(pairwise_iou)\n",
    "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
    "        paired_true += 1  # index is instance id - 1\n",
    "        paired_pred += 1  # hence return back to original\n",
    "    else:  # * Exhaustive maximal unique pairing\n",
    "        #### Munkres pairing with scipy library\n",
    "        # the algorithm return (row indices, matched column indices)\n",
    "        # if there is multiple same cost in a row, index of first occurence\n",
    "        # is return, thus the unique pairing is ensure\n",
    "        # inverse pair to get high IoU as minimum\n",
    "        paired_true, paired_pred = linear_sum_assignment(-pairwise_iou)\n",
    "        ### extract the paired cost and remove invalid pair\n",
    "        paired_iou = pairwise_iou[paired_true, paired_pred]\n",
    "\n",
    "        # now select those above threshold level\n",
    "        # paired with iou = 0.0 i.e no intersection => FP or FN\n",
    "        paired_true = list(paired_true[paired_iou > match_iou] + 1)\n",
    "        paired_pred = list(paired_pred[paired_iou > match_iou] + 1)\n",
    "        paired_iou = paired_iou[paired_iou > match_iou]\n",
    "\n",
    "    # get the actual FP and FN\n",
    "    unpaired_true = [idx for idx in true_id_list[1:] if idx not in paired_true]\n",
    "    unpaired_pred = [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n",
    "    # print(paired_iou.shape, paired_true.shape, len(unpaired_true), len(unpaired_pred))\n",
    "\n",
    "    #\n",
    "    tp = len(paired_true)\n",
    "    fp = len(unpaired_pred)\n",
    "    fn = len(unpaired_true)\n",
    "    # get the F1-score i.e DQ\n",
    "    dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n",
    "    # get the SQ, no paired has 0 iou so not impact\n",
    "    sq = paired_iou.sum() / (tp + 1.0e-6)\n",
    "\n",
    "    return [dq, sq, dq * sq], [paired_true, paired_pred, unpaired_true, unpaired_pred]\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "def get_dice_1(true, pred):\n",
    "    \"\"\"Traditional dice.\"\"\"\n",
    "    # cast to binary 1st\n",
    "    true = np.copy(true)\n",
    "    pred = np.copy(pred)\n",
    "    true[true > 0] = 1\n",
    "    pred[pred > 0] = 1\n",
    "    inter = true * pred\n",
    "    denom = true + pred\n",
    "    dice_score = 2.0 * np.sum(inter) / (np.sum(denom) + 0.0001)\n",
    "    if np.sum(inter)==0 and np.sum(denom)==0:\n",
    "        dice_score = 1 # to handel cases without any nuclei\n",
    "    #print(dice_score)\n",
    "    return dice_score\n",
    "\n",
    "#############################################################################################################\n",
    "def remap_label(pred, by_size=False):\n",
    "    \"\"\"Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3] \n",
    "    not [0, 2, 4, 6]. The ordering of instances (which one comes first) \n",
    "    is preserved unless by_size=True, then the instances will be reordered\n",
    "    so that bigger nucler has smaller ID.\n",
    "    Args:\n",
    "        pred    : the 2d array contain instances where each instances is marked\n",
    "                  by non-zero integer\n",
    "        by_size : renaming with larger nuclei has smaller id (on-top)\n",
    "    \"\"\"\n",
    "    pred_id = list(np.unique(pred))\n",
    "    pred_id.remove(0)\n",
    "    if len(pred_id) == 0:\n",
    "        return pred  # no label\n",
    "    if by_size:\n",
    "        pred_size = []\n",
    "        for inst_id in pred_id:\n",
    "            size = (pred == inst_id).sum()\n",
    "            pred_size.append(size)\n",
    "        # sort the id by size in descending order\n",
    "        pair_list = zip(pred_id, pred_size)\n",
    "        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n",
    "        pred_id, pred_size = zip(*pair_list)\n",
    "\n",
    "    new_pred = np.zeros(pred.shape, np.int32)\n",
    "    for idx, inst_id in enumerate(pred_id):\n",
    "        new_pred[pred == inst_id] = idx + 1\n",
    "    return new_pred\n",
    "\n",
    "def get_id_from_file_path(fp, indicator):\n",
    "    return os.path.basename(fp).replace(indicator, '')\n",
    "\n",
    "def chunker(seq, seq2, size):\n",
    "    return ([seq[i:i+size], seq2[i:i+size]] for i in range(0, len(seq), size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "294950f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Data Generator (fixed to yield float32 masks)\n",
    "def data_gen(list_files, list_masks, batch_size, p,\n",
    "            augment=False):\n",
    "    aug = albumentation_aug(p)\n",
    "    while True:\n",
    "        for batch_imgs, batch_msks in chunker(list_files, list_masks, batch_size):\n",
    "            X, Y = [], []\n",
    "            for img_p, m_p in zip(batch_imgs, batch_msks):\n",
    "                x = cv2.imread(img_p)\n",
    "                x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "                m = cv2.imread(m_p, cv2.IMREAD_GRAYSCALE)\n",
    "                # binarize mask (0 or 1)\n",
    "                m_bin = (m == 255).astype(np.uint8)\n",
    "\n",
    "                if augment:\n",
    "                    augd = aug(image=x, mask=m_bin)\n",
    "                    x, m_bin = augd['image'], augd['mask']\n",
    "\n",
    "                X.append(x / 255.0)\n",
    "                Y.append(m_bin)\n",
    "\n",
    "            # convert to float32 so loss/metrics see float32 * float32\n",
    "            X = np.array(X, dtype=np.float32)\n",
    "            Y = np.expand_dims(np.array(Y, dtype=np.float32), -1)\n",
    "\n",
    "            yield X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef54452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\albumentations\\core\\validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - dice_coef: 0.2277 - loss: -0.0175\n",
      "Epoch 1: val_dice_coef improved from -inf to 0.23214, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 4s/step - dice_coef: 0.2279 - loss: -0.0181 - val_dice_coef: 0.2321 - val_loss: -0.0606 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - dice_coef: 0.3051 - loss: -0.0621\n",
      "Epoch 2: val_dice_coef improved from 0.23214 to 0.33214, saving model to output_model/unet.weights.h5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4s/step - dice_coef: 0.3044 - loss: -0.0626 - val_dice_coef: 0.3321 - val_loss: -0.2139 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/30\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - dice_coef: 0.3431 - loss: -0.1138\n",
      "Epoch 3: val_dice_coef did not improve from 0.33214\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 5s/step - dice_coef: 0.3425 - loss: -0.1136 - val_dice_coef: 0.1935 - val_loss: -0.0620 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/30\n",
      "\u001b[1m25/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m30s\u001b[0m 4s/step - dice_coef: 0.2767 - loss: -0.1166"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m deep_unet(opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_of_channel\u001b[39m\u001b[38;5;124m'\u001b[39m], opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_LR\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m              \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                            \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_img\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_img\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch_num\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_drop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# load best\u001b[39;00m\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_save_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32me:\\my_files\\ML files\\YOLO_projects\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Cell 7: Train → Validation → Test Evaluation\n",
    "# callbacks\n",
    "logger     = tf.keras.callbacks.CSVLogger(opts['model_save_path'] + 'unet.log')\n",
    "lr_drop    = step_decay_schedule(\n",
    "                initial_lr   = opts['init_LR'],\n",
    "                decay_factor = opts['LR_decay_factor'],\n",
    "                epochs_drop  = opts['LR_drop_after_nth_epoch']\n",
    "            )\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    opts['model_save_path'] + 'unet.weights.h5',   # ← note the new extension\n",
    "    monitor='val_dice_coef',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# build model\n",
    "model = deep_unet(opts['number_of_channel'], opts['init_LR'])\n",
    "\n",
    "# train\n",
    "history = model.fit(\n",
    "    data_gen(train_img, train_mask, opts['batch_size'], 1,\n",
    "              augment=True),\n",
    "    validation_data = data_gen(val_img, val_mask, opts['batch_size'], 1,\n",
    "                            augment=False),\n",
    "    steps_per_epoch   = len(train_img)//opts['batch_size'],\n",
    "    validation_steps  = max(1, len(val_img)//opts['batch_size']),\n",
    "    epochs            = opts['epoch_num'],\n",
    "    callbacks         = [checkpoint, logger, lr_drop],\n",
    "    verbose           = 1\n",
    ")\n",
    "\n",
    "# load best\n",
    "model.load_weights(opts['model_save_path'] + 'unet.weights.h5')\n",
    "\n",
    "# evaluate on test\n",
    "preds = model.predict(\n",
    "    data_gen(test_img, test_mask, batch_size=1, p=1, augment=False),\n",
    "    steps=len(test_img)\n",
    ")\n",
    "preds_bin = (preds>opts['treshold']).astype('uint8')\n",
    "\n",
    "dice_scores = []\n",
    "for i, gt_p in enumerate(test_mask):\n",
    "    gt = cv2.imread(gt_p, cv2.IMREAD_GRAYSCALE)\n",
    "    pr = cv2.resize(preds_bin[i,...,0], (gt.shape[1], gt.shape[0]),\n",
    "                    interpolation=cv2.INTER_NEAREST)\n",
    "    dice_scores.append(get_dice_1(gt, pr))\n",
    "\n",
    "print(f\"Test Dice: {np.mean(dice_scores):.4f} ± {np.std(dice_scores):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
